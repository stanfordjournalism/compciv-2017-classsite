********************
Data Janitorial Work
********************

Choose a real-world public dataset of significant size and messiness. Do the research and journalism needed to understand the dataset's flaws, as well as the scope of possible/relevant data-cleaning methods.

This will be one of the more difficult assignments, but one that will most resemble the kind of dogged work in real-life journalism.


Rubric
======

Due date:
    Approximately Week 5/Mid-February

.. csv-table::
    :header: "Points", "Metric"
    :widths: 10, 90

    2,A dataset consisting of multiple data files/tables
    2,An efficient bootstrap script
    1,Trimming or consolidation of unnecessary columns
    3,Reconciliation
    3,Stack/joining/collating the data files
    3,Writeup documenting dataset's origin and usage
    1,Tidied data
    1,Deployed to Github


The work
========


.. note:: Under construction!

    This is one of the harder projects to describe and also to scope in such a way that makes it "doable" while being journalistically useful and fun. Also, it's alot easier to describe the steps after everyone has some programming experience with Python.


Fragmented dataset
------------------


By a "fragmented" dataset, I simply mean a dataset that is not completely contained in a single file. There are real-world reasons for this. Institutions change over time, and so does what data they choose to collect, and the methodology of that data collection.


An easy example is the `California Department of Education's Postsecondary Preparation data site <http://www.cde.ca.gov/ds/sp/ai/>`, which contains more than a decade of SAT and ACT scores. The problem is that the files are split up between years. And, more importantly, these tests have changed over the years so that year-over-year comparison may not even be *relevant*.

And on top of that, the data format and titles have changed.


Be prepared for this step to be time-consuming. My hope is that throughout this quarter, you're on the lookout for datasets that are relevant to your interests, which makes it much easier to figure out a way to clean and analyze them for stories.



Documenting the data
--------------------

You will have to do research to make sense of this data. Documenting that research is the best way to get the most out of research.

Be prepared to do a writeup of no less than 800 words, and one that includes an informal bibliography of:

- All the official documentation for a dataset, including forms, data schemas, and user manuals that have been generated by a governmental agency.
- At least 5 news stories that reference the dataset. This is very helpful as a way to double-checking your work.



Collating the data
------------------

No matter how your dataset is split, your work should result in a single, easy-to-use data table. Collation can include joining two different data tables by a key field. Or "stacking" data files from different years.


Reconciling the data
--------------------

By "reconciliation", I mean doing the work so that separate years/epochs of a dataset can be compared apples-to-apples. Easier said than done.


A bootstrap script
------------------

This project should include a script named something like ``bootstrap.py``, that contains all the steps needed by another user to reproduce the data-gathering process.

This script should demonstrate some knowledge of that hard computer science problem of "cache invalidation". Or, it should at least be **idempotent**: if a user has already run the script and gotten the data, the `bootstrap.py` script *should not* try to re-download the data unless it is necessary.

Tidied data
-----------

You should produce a dataset that follows the concept of "tidy data": `<http://r4ds.had.co.nz/tidy.html>`_



